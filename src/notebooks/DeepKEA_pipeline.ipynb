{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e51234",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "1. Creating the Anaconda environement /tool_keaml/src/examples/keyword_extraction_environment.yml\n",
    "2. After selecting the right Kernel for the jupyter notebook based on the created anaconda env, it should be possible to run the following codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e44eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/rtue/playground/jupyter-notebook/workspace/codes\")\n",
    "sys.path.append(\"/home/rtue/playground/jupyter-notebook/\")\n",
    "sys.path.append(\"/home/rtue/playground/python_projects/tool_keaml/src/\")\n",
    "sys.path.append(\"/home/rtue/playground/python_projects/tool_keaml/src/utils\")\n",
    "sys.path.append(\"/home/rtue/playground/jupyter-notebook/keyword_extraction\")\n",
    "import util\n",
    "from training_data.data_prepare import DataPrepare\n",
    "import prepare_input_for_YouTubeDNN as prepare_input_for_dnn\n",
    "import youTube_pipeline\n",
    "import encoding_pipeline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from termcolor import colored\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import re, nltk\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, time, ast, collections, pickle, faiss\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools, time\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from tensorflow.python.keras import backend as K\n",
    "from deepmatch.models import YoutubeDNN\n",
    "from tensorflow.python.keras.layers import Embedding, Lambda\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras import initializers\n",
    "from deepctr import feature_column as fc_lib\n",
    "from deepmatch.utils import sampledsoftmaxloss\n",
    "from deepmatch.utils import recall_N\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from deepctr.feature_column import build_input_features\n",
    "from deepctr.layers import DNN\n",
    "from deepctr.layers.utils import NoMask, combined_dnn_input\n",
    "from tensorflow.python.keras.models import Model\n",
    "from deepmatch.layers import PoolingLayer\n",
    "from deepmatch.utils import get_item_embedding\n",
    "from deepmatch.inputs import input_from_feature_columns, create_embedding_matrix\n",
    "from deepmatch.layers.core import SampledSoftmaxLayer, EmbeddingIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7c83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"AI-Growth-Lab/PatentSBERTa\")\n",
    "embedding_dim=768\n",
    "max_len_token=model.get_max_seq_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e016ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dw_id_kw='/home/rtue/playground/python_projects/resources/df_np_id_kw.csv'\n",
    "path_dw_id_tit_abst_claim ='/home/rtue/playground/python_projects/resources/df_np_id_tit_abst_claim_detden.csv'\n",
    "\n",
    "lst_columns=['TIEN', 'ABEN', 'CLMEN']\n",
    "lst_columns_all=[\"PPID\",\"TIEN\", \"ABEN\", \"CLMEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239fc2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPrepare object has the following properties:\n",
      " df_full_text, dict_full_text, df_der_text, dict_der_text\n",
      "rows_with_NaN: Empty DataFrame\n",
      "Columns: [PPID, KEYWORD]\n",
      "Index: []\n",
      "rows_with_NaN: Empty DataFrame\n",
      "Columns: [PPID, TIEN, ABEN, CLMEN, DETDEN]\n",
      "Index: []\n",
      "data_kw is merged version and len(data_kw): 271137\n",
      "occurance dictionary len: 79450\n",
      "15249\n",
      "15239\n",
      "25534\n",
      "14244\n"
     ]
    }
   ],
   "source": [
    "data=DataPrepare()\n",
    "\n",
    "data.set_der_abst()\n",
    "data.set_full_text()\n",
    "\n",
    "data_id_kw, data_id_tit_abs_clm, data_kw_or=util.read_dataset(path_dw_id_kw, path_dw_id_tit_abst_claim)\n",
    "\n",
    "dict_or_occ_kw=util.get_occurance_dict(data_kw_or,['KEYWORD'])\n",
    "\n",
    "print(len(data.df_full_text))\n",
    "print(len(data.dict_full_text))\n",
    "print(len(data.df_der_text))\n",
    "print(len(data.dict_der_text))\n",
    "#data.df_full_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d96fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original len df: 271137\n",
      "removing rows from the df\n",
      "after removing len df: 14957\n",
      "occurance dictionary len: 77\n"
     ]
    }
   ],
   "source": [
    "#FILTER DATA (prune the keywords)\n",
    "\n",
    "list_ele_1=util.get_list_ele_len_less_than(dict_or_occ_kw)\n",
    "list_ele_3 = util.get_list_pha_len_less(dict_or_occ_kw)\n",
    "list_ele_4 = util.get_list_ele_occ_more_than(dict_or_occ_kw)\n",
    "\n",
    "#set parameters\n",
    "n_less=100#, 70, 50]\n",
    "lst_n_sample=[70]#[10, 20, 50, 70]\n",
    "lst_n_epoch=[23]#23,30]\n",
    "    \n",
    "list_ele_2=util.get_list_ele_occ_less_than(dict_or_occ_kw, n_less)\n",
    "\n",
    "lst_all=set(list_ele_1 + list_ele_2+list_ele_3+list_ele_4)\n",
    "\n",
    "print('original len df:', len(data_kw_or))\n",
    "data_kw=util.remove_rows_contain_kw_in_list( lst_all,data_kw_or)\n",
    "print('after removing len df:', len(data_kw))\n",
    "dict_occ_kw_clean=util.get_occurance_dict(data_kw,['KEYWORD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769b7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique KWs: \u001b[32m77\u001b[0m\n",
      "False\n",
      "False\n",
      "False\n",
      "14957\n"
     ]
    }
   ],
   "source": [
    "#encode the data\n",
    "data_kw_copy, embedding_matrix_tien, embedding_matrix_abst, embedding_matrix_clm= encoding_pipeline.generate_embeddings(data_kw.copy(), data, model)\n",
    "\n",
    "data_kw_copy, lbe_ppid, lbe_kw = encode_with_label_encoder(data_kw_copy)\n",
    "\n",
    "embedding_matrix_kw=encoding_pipeline.generate_kw_emb(lbe_kw, model)\n",
    "\n",
    "print(np.isnan(embedding_matrix_tien).any())\n",
    "print(np.isnan(embedding_matrix_abst).any())\n",
    "print(np.isnan(embedding_matrix_clm).any())\n",
    "print(len(data_kw_copy['KEYWORD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efef6842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5356/5356 [00:00<00:00, 6404.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_set[0]): 6 len(test_set[0]): 6\n",
      "len(train_set): 13586 len(test_set): 1371\n",
      "*******************************\n",
      "train_set len: 13586\n",
      "test_set len: 1371\n",
      "train_model_input len: 5\n",
      "test_model_input len: 5\n",
      "*******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = gen_train_test_split_and_generate_test_data(data_kw_copy, lst_columns,  m_kws=11, negsample=0)#MAKE IT 11\n",
    "\n",
    "train_model_input, train_label = gen_model_input(train_set)\n",
    "test_model_input, test_label = gen_model_input(test_set)\n",
    "\n",
    "\n",
    "print('*******************************')\n",
    "print('train_set len:', len(train_set))\n",
    "print('test_set len:', len(test_set))\n",
    "\n",
    "train_model_input, train_label = gen_model_input(train_set)\n",
    "test_model_input, test_label = gen_model_input(test_set)\n",
    "\n",
    "print('train_model_input len:', len(train_model_input))\n",
    "print('test_model_input len:', len(test_model_input))\n",
    "\n",
    "print('*******************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b495463",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dict=dict()\n",
    "for e in train_set:\n",
    "    id_=e[0]\n",
    "    label=e[1]\n",
    "    \n",
    "    #print(e)\n",
    "    if id_ in train_set_dict:\n",
    "        train_set_dict[id_]+=[label]\n",
    "    else:\n",
    "        train_set_dict[id_]=[label]\n",
    "        \n",
    "test_set_dict=dict()\n",
    "for e in test_set:\n",
    "    id_=e[0]\n",
    "    label=e[1]\n",
    "    \n",
    "    #print(e)\n",
    "    if id_ in test_set_dict:\n",
    "        test_set_dict[id_]+=[label]\n",
    "    else:\n",
    "        test_set_dict[id_]=[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1961725",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_profile = data_kw_copy[lst_columns_all].drop_duplicates('PPID')\n",
    "kw_profile = data_kw_copy[[\"KEYWORD\"]].drop_duplicates('KEYWORD')\n",
    "\n",
    "doc_profile.set_index(\"PPID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd02b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************START*********************\n",
      "th_for filtering kws: 100  n_neg samples: 70  epoch_n: 23\n",
      "I am in train_model function\n",
      "WARNING:tensorflow:From /home/rtue/anaconda3/envs/snorkel_env_keyterm_clone/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rtue/anaconda3/envs/snorkel_env_keyterm_clone/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rtue/anaconda3/envs/snorkel_env_keyterm_clone/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rtue/anaconda3/envs/snorkel_env_keyterm_clone/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in recall_N_after_post_processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:01, 76.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit 1114\n",
      "recall 0.8297619276692807\n",
      "precision 0.5911065271587718\n",
      "f_score: 0.6903914148653713\n",
      "len rows 100\n",
      "saved the dataframe\n",
      "0 th_for filtering kws: 100  n_neg samples: 70  epoch_n: 23 n_pred_: 20\n",
      "**************FINISHED******************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "\n",
    "    for n_sample in lst_n_sample:\n",
    "\n",
    "        for n_epoch in lst_n_epoch:\n",
    "\n",
    "            print('**************START*********************')\n",
    "            print('th_for filtering kws:',n_less, ' n_neg samples:', n_sample, ' epoch_n:',  n_epoch)\n",
    "\n",
    "            user_embs, item_embs=youTube_pipeline.train_model(data_kw_copy,train_model_input, train_label, test_model_input, \n",
    "                                                                embedding_matrix_tien, embedding_matrix_abst, embedding_matrix_clm,\n",
    "                                                                n_sample, n_epoch )\n",
    "            \n",
    "            \n",
    "            n_pred_=20\n",
    "            p, r, f =youTube_pipeline.recall_N_after_post_processing(test_set_dict, kw_profile, data, data_id_tit_abs_clm, lbe_ppid,lbe_kw, user_embs, item_embs,  n_total=len(dict_occ_kw_clean),n_pred_=20)\n",
    "\n",
    "            print(i,'th_for filtering kws:',n_less, ' n_neg samples:', n_sample, ' epoch_n:',  n_epoch, 'n_pred_:', n_pred_)\n",
    "            print('**************FINISHED******************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e49a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_test_split_and_generate_test_data(data_kw, columns_feature, m_kws=10, negsample=0):\n",
    "\n",
    "    item_ids = data_kw['PPID'].unique()\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    test_set_n_kws=m_kws\n",
    "    \n",
    "    \n",
    "    data_kw_suffle=data_kw.sample(frac=1)\n",
    "    data_kw_suffle.head()\n",
    "    \n",
    "    for index, (reviewerID, hist) in enumerate (tqdm(data_kw_suffle.groupby('PPID'))):\n",
    "\n",
    "        pos_list = hist['KEYWORD'].tolist()\n",
    "        tit='', \n",
    "        abst='', \n",
    "        clm='', \n",
    "        detden=''\n",
    "        \n",
    "        if 'TIEN' in columns_feature:\n",
    "            tit= list(hist['TIEN'])[0]\n",
    "        if 'ABEN' in columns_feature:\n",
    "            abst= list(hist['ABEN'])[0]\n",
    "        if 'CLMEN' in columns_feature:    \n",
    "            clm= list(hist['CLMEN'])[0]\n",
    "        if 'DETDEN' in columns_feature:    \n",
    "            detden= list(hist['DETDEN'])[0]\n",
    "\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
    "            \n",
    "        set_test_ids=set([item[0] for item in test_set])\n",
    "        \n",
    "        if len(set_test_ids)<100 and len(pos_list)>=test_set_n_kws:\n",
    "            for i in range(0, len(pos_list)):\n",
    "                test_set.append((reviewerID, pos_list[i], 1,tit ,abst, clm))\n",
    "        else:\n",
    "            for i in range(0, len(pos_list)):\n",
    "                train_set.append((reviewerID, pos_list[i], 1,tit ,abst, clm))\n",
    "                \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print('len(train_set[0]):',len(train_set[0]),'len(test_set[0]):',len(test_set[0]))\n",
    "    print('len(train_set):',len(train_set), 'len(test_set):',len(test_set))\n",
    "\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73f21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_input(train_set):\n",
    "    \n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_iid = np.array([line[1] for line in train_set])#movie_id\n",
    "    train_label = np.array([line[2] for line in train_set])\n",
    "    \n",
    "    train_tien_seq =np.array( [line[3] for line in train_set]) #title\n",
    "    \n",
    "    train_abst_seq = np.array([line[4] for line in train_set])#abst\n",
    "\n",
    "    train_clm_seq = np.array([line[5] for line in train_set])#clm\n",
    "\n",
    "\n",
    "    train_model_input = {\"PPID\": train_uid,\"KEYWORD\": train_iid,  \n",
    "                         \"tien\":train_tien_seq,\n",
    "                         \"aben\":train_abst_seq,\n",
    "                        \"clmen\":train_clm_seq}\n",
    "    \n",
    "    return train_model_input, train_label\n",
    "\n",
    "\n",
    "def encode_with_label_encoder(df):\n",
    "    data_kw_copy=df.copy()\n",
    "    \n",
    "    features = ['PPID', 'KEYWORD']\n",
    "    feature_max_idx = {}\n",
    "\n",
    "    lbe_ppid = LabelEncoder()\n",
    "    lbe_kw = LabelEncoder()\n",
    "\n",
    "    data_kw_copy['PPID'] = lbe_ppid.fit_transform(data_kw_copy['PPID']) \n",
    "    data_kw_copy['KEYWORD'] = lbe_kw.fit_transform(data_kw_copy['KEYWORD'])\n",
    "\n",
    "    print('number of unique KWs:', colored(data_kw_copy['KEYWORD'].max()+1, 'green'))\n",
    "    \n",
    "    return data_kw_copy, lbe_ppid, lbe_kw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_dict_items(dict_t):\n",
    "    dict_res= dict()\n",
    "    \n",
    "    for i, (uid, gt) in enumerate(test_set_dict.items()):\n",
    "        try:\n",
    "            id_real=lbe_ppid.inverse_transform([uid])[0]\n",
    "            set_gt_decoded=set()\n",
    "\n",
    "            for kw in gt:\n",
    "                kw_str=lbe_kw.inverse_transform([kw])[0]\n",
    "                set_gt_decoded.add(kw_str)\n",
    "            \n",
    "            dict_res[id_real]=set_gt_decoded\n",
    "        except:\n",
    "            print('exception in:',i)\n",
    "            sys.exit()  \n",
    "    \n",
    "    return dict_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkel_env_keyterm_clone]",
   "language": "python",
   "name": "conda-env-snorkel_env_keyterm_clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
